 
---

# âš™ï¸ LangChain Runnable æ•™ç¨‹ç¬”è®°

## ğŸ“‘ ç›®å½•

* [1. ä»€ä¹ˆæ˜¯ Runnable](#1-ä»€ä¹ˆæ˜¯-runnable)
* [2. `invoke`ï¼ˆå•æ¬¡è°ƒç”¨ï¼‰](#2-invokeå•æ¬¡è°ƒç”¨)
* [3. `batch`ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰](#3-batchå¹¶è¡Œå¤„ç†)
* [4. `batch_as_completed`ï¼ˆä¹±åºå¹¶è¡Œï¼‰](#4-batch_as_completedä¹±åºå¹¶è¡Œ)
* [5. å¼‚æ­¥è°ƒç”¨ï¼ˆasyncï¼‰](#5-å¼‚æ­¥è°ƒç”¨async)
* [6. æµå¼è°ƒç”¨ï¼ˆstream / astreamï¼‰](#6-æµå¼è°ƒç”¨stream--astream)
* [7. è¾“å…¥å’Œè¾“å‡ºç±»å‹](#7-è¾“å…¥å’Œè¾“å‡ºç±»å‹)
* [8. RunnableConfigï¼ˆè¿è¡Œæ—¶é…ç½®ï¼‰](#8-runnableconfigè¿è¡Œæ—¶é…ç½®)
* [9. è‡ªå®šä¹‰ Runnable](#9-è‡ªå®šä¹‰-runnable)
* [10. å¯é…ç½® Runnables](#10-å¯é…ç½®-runnables)
* [11. Streaming APIsï¼ˆæµå¼æ¥å£ï¼‰](#11-streaming-apisæµå¼æ¥å£)
* [12. Inspecting Schemasï¼ˆæ£€æŸ¥è¾“å…¥è¾“å‡ºç»“æ„ï¼‰](#12-inspecting-schemasæ£€æŸ¥è¾“å…¥è¾“å‡ºç»“æ„)
* [13. with\_typesï¼ˆæ‰‹åŠ¨æŒ‡å®šç±»å‹ï¼‰](#13-with_typesæ‰‹åŠ¨æŒ‡å®šç±»å‹)
* [14. RunnableConfig ä¼ é€’ï¼ˆPropagationï¼‰](#14-runnableconfig-ä¼ é€’propagation)
* [15. é…ç½®é€‰é¡¹è¯¦è§£](#15-é…ç½®é€‰é¡¹è¯¦è§£)

  * [15.1 run\_name / run\_id](#151-run_name--run_id)
  * [15.2 tags / metadata](#152-tags--metadata)
  * [15.3 recursion\_limit](#153-recursion_limit)
  * [15.4 max\_concurrency](#154-max_concurrency)
  * [15.5 configurable](#155-configurable)
  * [15.6 callbacks](#156-callbacks)
* [16. ä»å‡½æ•°åˆ›å»º Runnable](#16-ä»å‡½æ•°åˆ›å»º-runnable)
* [17. å¯é…ç½® Runnablesï¼ˆConfigurable Runnablesï¼‰](#17-å¯é…ç½®-runnablesconfigurable-runnables)
* [18. æ€»ç»“](#18-æ€»ç»“)

---

## 1. ä»€ä¹ˆæ˜¯ Runnable

`Runnable` æ˜¯ **LangChain çš„ç»Ÿä¸€æ‰§è¡Œæ¥å£**ï¼Œå‡ ä¹æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼ˆLLMã€ChatModelã€Promptã€Retrieverã€OutputParserã€LangGraph ç­‰ï¼‰éƒ½å®ç°äº†å®ƒã€‚

### å¸¸ç”¨æ–¹æ³•

* `invoke`ï¼šå•è¾“å…¥ â†’ å•è¾“å‡º
* `batch`ï¼šå¤šä¸ªè¾“å…¥å¹¶è¡Œå¤„ç† â†’ å¤šä¸ªè¾“å‡º
* `stream`ï¼šæµå¼è¾“å‡º
* `async`ï¼šå¼‚æ­¥è°ƒç”¨
* `compose`ï¼šç»„åˆå¤šä¸ª Runnable

### ç¤ºä¾‹

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")
result = llm.invoke("Hello, how are you?")
print(result.content)
```

---

## 2. `invoke`ï¼ˆå•æ¬¡è°ƒç”¨ï¼‰

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

# å•æ¬¡è°ƒç”¨
resp = llm.invoke("è¯·ç”¨ä¸€å¥è¯æ€»ç»“ï¼šäººå·¥æ™ºèƒ½çš„æ ¸å¿ƒä»·å€¼")
print(resp.content)
```

---

## 3. `batch`ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

# å¹¶è¡Œå¤„ç†å¤šä¸ªè¾“å…¥
questions = ["ä»€ä¹ˆæ˜¯AIï¼Ÿ", "é‡å­è®¡ç®—çš„å‰æ™¯ï¼Ÿ", "å¤ªé˜³èƒ½çš„ä¼˜åŠ¿ï¼Ÿ"]
responses = llm.batch(questions)

for r in responses:
    print(r.content)
```

---

## 4. `batch_as_completed`ï¼ˆä¹±åºå¹¶è¡Œï¼‰

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

questions = ["A", "B", "C"]

for resp in llm.batch_as_completed(questions):
    print(resp.content)  # æŒ‰å®Œæˆé¡ºåºè¿”å›ï¼Œè€Œä¸æ˜¯è¾“å…¥é¡ºåº
```

---

## 5. å¼‚æ­¥è°ƒç”¨ï¼ˆasyncï¼‰

```python
import asyncio
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

async def main():
    result = await llm.ainvoke("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
    print(result.content)

asyncio.run(main())
```

---

## 6. æµå¼è°ƒç”¨ï¼ˆstream / astreamï¼‰

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

# åŒæ­¥æµå¼è¾“å‡º
for chunk in llm.stream("è¯·é€æ­¥è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†"):
    print(chunk.content, end="", flush=True)
```

---

## 7. è¾“å…¥å’Œè¾“å‡ºç±»å‹

```python
print("è¾“å…¥æ¨¡å¼:", llm.input_schema.schema())
print("è¾“å‡ºæ¨¡å¼:", llm.output_schema.schema())
```

---

## 8. RunnableConfigï¼ˆè¿è¡Œæ—¶é…ç½®ï¼‰

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

config = {"tags": ["demo"], "metadata": {"topic": "AI"}}
resp = llm.invoke("è§£é‡Šå¤§è¯­è¨€æ¨¡å‹çš„ç”¨é€”", config=config)
print(resp.content)
```

---

## 9. è‡ªå®šä¹‰ Runnable

```python
from langchain_core.runnables import RunnableLambda

# è‡ªå®šä¹‰ä¸€ä¸ªç¿»å€å‡½æ•°
double = RunnableLambda(lambda x: x * 2)
print(double.invoke(10))  # 20
```

---

## 10. å¯é…ç½® Runnables

```python
from langchain_core.runnables import RunnableConfigurableFields
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

configurable_llm = RunnableConfigurableFields(
    llm, model="gpt-4o-mini", temperature=0.7
)

resp = configurable_llm.invoke("ç®€è¿°å¼ºåŒ–å­¦ä¹ ")
print(resp.content)
```

---

## 11. Streaming APIsï¼ˆæµå¼æ¥å£ï¼‰

* `stream / astream`ï¼šå¸¸è§æµå¼è¾“å‡º
* `astream_events`ï¼šé«˜çº§äº‹ä»¶æµ
* `astream_log`ï¼šæ—§ç‰ˆ API

---

## 12. Inspecting Schemasï¼ˆæ£€æŸ¥è¾“å…¥è¾“å‡ºç»“æ„ï¼‰

```python
print("è¾“å…¥:", llm.get_input_schema())
print("è¾“å‡º:", llm.get_output_schema())
print("é…ç½®:", llm.config_schema())
```

---

## 13. with\_typesï¼ˆæ‰‹åŠ¨æŒ‡å®šç±»å‹ï¼‰

```python
chain = (llm).with_types(input_type=dict, output_type=str)
```

---

## 14. RunnableConfig ä¼ é€’ï¼ˆPropagationï¼‰

* LCEL ç»„åˆï¼šè‡ªåŠ¨ä¼ æ’­é…ç½®
* Python 3.9/3.10 å¼‚æ­¥ï¼šéœ€æ‰‹åŠ¨ä¼ é€’

---

## 15. é…ç½®é€‰é¡¹è¯¦è§£

### 15.1 run\_name / run\_id

```python
config = {"run_name": "demo", "run_id": "1234"}
llm.invoke("ä»‹ç»æœºå™¨å­¦ä¹ ", config=config)
```

### 15.2 tags / metadata

```python
config = {"tags": ["education"], "metadata": {"course": "AI"}}
```

### 15.3 recursion\_limit

é™åˆ¶é€’å½’å±‚æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯ã€‚

### 15.4 max\_concurrency

è®¾ç½®æœ€å¤§å¹¶å‘æ•°ã€‚

### 15.5 configurable

ä¼ é€’è¿è¡Œæ—¶å‚æ•°ã€‚

### 15.6 callbacks

è®¾ç½®å›è°ƒå‡½æ•°ã€‚

---

## 16. ä»å‡½æ•°åˆ›å»º Runnable

```python
from langchain_core.runnables import RunnableLambda

def square(x): return x * x
square_runnable = RunnableLambda(square)
print(square_runnable.invoke(5))  # 25
```

---

## 17. å¯é…ç½® Runnablesï¼ˆConfigurable Runnablesï¼‰

```python
from langchain_core.runnables import ConfigurableField

configurable_llm = llm.configurable_fields(
    model=ConfigurableField(id="llm_model")
)

resp = configurable_llm.invoke("è§£é‡Šæ·±åº¦å¼ºåŒ–å­¦ä¹ ", config={"configurable": {"llm_model": "gpt-4"}})
print(resp.content)
```

---

## 18. æ€»ç»“

* **Runnable = ç»Ÿä¸€æ‰§è¡Œæ¥å£**
* å¸¸ç”¨æ–¹æ³•ï¼š`invoke / batch / stream / async`
* **é…ç½®çµæ´»**ï¼š`RunnableConfig`
* **é«˜çº§åŠŸèƒ½**ï¼šStreamingã€Schema æ£€æŸ¥ã€Configurable Runnables

ğŸ‘‰ Runnable æ˜¯ **LangChain çš„æ ¸å¿ƒæ‰§è¡ŒæŠ½è±¡**ï¼ŒæŒæ¡å®ƒå°±èƒ½è‡ªç”±ç»„åˆå’Œæ‰©å±•å„ç§é“¾å¼ä»»åŠ¡ã€‚

---

 