
## 1. ä»€ä¹ˆæ˜¯ Runnable

**æ¦‚å¿µ**
`Runnable` æ˜¯ LangChain çš„ç»Ÿä¸€æ¥å£ï¼Œå‡ ä¹æ‰€æœ‰ç»„ä»¶éƒ½å®ç°äº†å®ƒï¼ˆLLMã€ChatModelã€Promptã€Retrieverã€OutputParserã€LangGraph Graph ç­‰ï¼‰ã€‚
å®ƒå®šä¹‰äº†ä¸€å¥—æ ‡å‡†æ–¹æ³•ï¼Œè®©æˆ‘ä»¬ç”¨ä¸€è‡´çš„æ–¹å¼å»è°ƒç”¨è¿™äº›ç»„ä»¶ã€‚

**æ ¸å¿ƒèƒ½åŠ›**

* `invoke`ï¼šå•è¾“å…¥ â†’ å•è¾“å‡º
* `batch`ï¼šå¤šä¸ªè¾“å…¥å¹¶è¡Œå¤„ç† â†’ å¤šä¸ªè¾“å‡º
* `stream`ï¼šæµå¼è¾“å‡º
* `inspect`ï¼šæŸ¥çœ‹è¾“å…¥/è¾“å‡ºç±»å‹å’Œé…ç½®
* `compose`ï¼šæŠŠå¤šä¸ª Runnable ç»„åˆæˆ pipeline

**ç¤ºä¾‹**

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

# Runnable çš„é€šç”¨æ–¹æ³• invoke
result = llm.invoke("Hello, how are you?")
print(result.content)
```

---

## 2. `invoke`ï¼ˆå•æ¬¡è°ƒç”¨ï¼‰

* æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼šè¾“å…¥ä¸€ä¸ªæ•°æ®ï¼Œè¿”å›ä¸€ä¸ªç»“æœã€‚
* è¾“å…¥å’Œè¾“å‡ºç±»å‹å–å†³äºå…·ä½“ç»„ä»¶ã€‚

```python
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_template("Translate this into French: {text}")
llm = ChatOpenAI(model="gpt-3.5-turbo")

chain = prompt | llm   # LCEL ç»„åˆæˆä¸€ä¸ª runnable

print(chain.invoke({"text": "I love programming"}).content)
```

---

## 3. `batch`ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰

* å¤„ç†å¤šä¸ªè¾“å…¥ï¼Œç»“æœé¡ºåºä¸è¾“å…¥ä¸€è‡´ã€‚
* é»˜è®¤ç”¨çº¿ç¨‹æ±  â†’ æé«˜ I/O æ€§èƒ½ï¼ˆæ¯”å¦‚è°ƒç”¨ APIï¼‰ã€‚

```python
texts = ["Hello", "How are you?", "Good night"]

results = llm.batch(texts)
for r in results:
    print(r.content)
```

---

## 4. `batch_as_completed`ï¼ˆä¹±åºå¹¶è¡Œï¼‰

* è¿”å›ç»“æœæ—¶ä¸ä¿è¯é¡ºåºï¼Œä½†ä¼šé™„å¸¦ indexã€‚
* é€‚åˆâ€œå…ˆå®Œæˆå…ˆè¿”å›â€çš„åœºæ™¯ã€‚

```python
for idx, res in llm.batch_as_completed(texts):
    print(f"Input {idx} â†’ {res.content}")
```

---

## 5. å¼‚æ­¥è°ƒç”¨ï¼ˆasyncï¼‰

* æ‰€æœ‰ä¸»è¦æ–¹æ³•éƒ½æœ‰å¼‚æ­¥ç‰ˆæœ¬ï¼ˆå‰ç¼€ `a`ï¼‰ã€‚

  * `ainvoke`
  * `abatch`
  * `astream`
  * `abatch_as_completed`

```python
import asyncio

async def main():
    result = await llm.ainvoke("Tell me a joke about AI")
    print(result.content)

asyncio.run(main())
```

---

## 6. æµå¼è°ƒç”¨ï¼ˆstream / astreamï¼‰

* ä¸€è¾¹ç”Ÿæˆä¸€è¾¹è¿”å›ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚
* å¸¸è§äº LLM è¾“å‡ºã€‚

```python
for chunk in llm.stream("Write a poem about the ocean."):
    print(chunk.content, end="")
```

---

## 7. è¾“å…¥å’Œè¾“å‡ºç±»å‹

ä¸åŒç»„ä»¶çš„è¾“å…¥è¾“å‡ºç±»å‹ä¸ä¸€æ ·ï¼š

* **Prompt**ï¼šè¾“å…¥ dict â†’ è¾“å‡º PromptValue
* **ChatModel**ï¼šè¾“å…¥ string/list â†’ è¾“å‡º ChatMessage
* **LLM**ï¼šè¾“å…¥ string/list â†’ è¾“å‡º string
* **Retriever**ï¼šè¾“å…¥ string â†’ è¾“å‡º List\[Document]
* **OutputParser**ï¼šè¾“å…¥ LLM è¾“å‡º â†’ è¾“å‡ºè‡ªå®šä¹‰ç±»å‹

---

## 8. RunnableConfigï¼ˆè¿è¡Œæ—¶é…ç½®ï¼‰

è°ƒç”¨ `.invoke()` ç­‰æ–¹æ³•æ—¶ï¼Œå¯ä»¥ä¼  `config`ï¼Œæ§åˆ¶è¿è¡Œå‚æ•°ã€‚

```python
result = llm.invoke(
    "Hello",
    config={
        "run_name": "test_run",
        "tags": ["demo"],
        "metadata": {"topic": "greeting"}
    }
)
```

å¸¸ç”¨é…ç½®ï¼š

* `run_name`ï¼šè¿è¡Œåå­—
* `tags`ï¼šæ ‡ç­¾
* `metadata`ï¼šå…ƒæ•°æ®
* `max_concurrency`ï¼šé™åˆ¶å¹¶è¡Œæ•°é‡
* `callbacks`ï¼šå›è°ƒå‡½æ•°ï¼ˆå¦‚æ—¥å¿—ã€è¿½è¸ªï¼‰

---

## 9. è‡ªå®šä¹‰ Runnable

å¦‚æœä½ è¦æ’å…¥è‡ªå®šä¹‰é€»è¾‘ï¼Œå¯ä»¥ç”¨ï¼š

* `RunnableLambda`ï¼šç®€å•å‡½æ•°åŒ…è£…
* `RunnableGenerator`ï¼šæ”¯æŒæµå¼çš„å‡½æ•°åŒ…è£…

```python
from langchain_core.runnables import RunnableLambda

def to_upper(x: str) -> str:
    return x.upper()

upper_runnable = RunnableLambda(to_upper)

print(upper_runnable.invoke("hello"))  # è¾“å‡º "HELLO"
```

---

## 10. å¯é…ç½® Runnables

å…è®¸ä½ åœ¨è¿è¡Œæ—¶è°ƒæ•´é…ç½®ï¼Œæ¯”å¦‚åˆ‡æ¢æ¨¡å‹æˆ–æ”¹æ¸©åº¦ã€‚

```python
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# é€šè¿‡ configurable_fields æš´éœ²å¯è°ƒå‚æ•°
llm = llm.configurable_fields("temperature")
```

---

ğŸ“Œ æ€»ç»“ï¼š
Runnable å°±æ˜¯ **LangChain çš„ç»Ÿä¸€æ‰§è¡Œæ¥å£**ï¼Œæ ¸å¿ƒæ–¹æ³•æ˜¯ `invoke / batch / stream / async`ï¼Œå†åŠ ä¸Šé…ç½® `RunnableConfig` å’Œå¯ç»„åˆæ€§ã€‚

---
 
ğŸ‘Œ å¥½çš„ï¼Œæˆ‘ä»¬ç»§ç»­æ‹†è§£ **Runnable çš„çŸ¥è¯†ç‚¹**ã€‚è¿™æ¬¡æ¶‰åŠåˆ° **Streaming APIs â†’ è¾“å…¥è¾“å‡ºç±»å‹ â†’ Schema æ£€æŸ¥ â†’ RunnableConfig â†’ é…ç½®ä¼ æ’­**ã€‚æˆ‘ä¼šé€æ®µè®²è§£å¹¶ç»™ä»£ç ç¤ºä¾‹ã€‚

---

## 1. Streaming APIsï¼ˆæµå¼è¾“å‡ºæ¥å£ï¼‰

æµå¼éå¸¸é‡è¦ï¼Œå®ƒèƒ½è®© LLM åº”ç”¨å¯¹ç”¨æˆ·â€œå³æ—¶å“åº”â€ã€‚Runnable æä¾›äº† 3 ç§æµå¼æ¥å£ï¼š

1. **stream / astream**

   * åŒæ­¥ `stream`ï¼Œå¼‚æ­¥ `astream`ã€‚
   * ä¸€è¾¹ç”Ÿæˆä¸€è¾¹è¿”å›ã€‚

   ```python
   for chunk in llm.stream("Write a poem about the stars."):
       print(chunk.content, end="")
   ```

2. **astream\_events**

   * é«˜çº§ APIï¼Œå¯ä»¥æµå¼è¿”å› **ä¸­é—´æ­¥éª¤ + æœ€ç»ˆç»“æœ**ã€‚
   * å¸¸ç”¨äºå¤æ‚ chain/agentã€‚

3. **astream\_logï¼ˆå·²è¿‡æ—¶ï¼‰**

   * è€ç‰ˆæœ¬çš„äº‹ä»¶æµ APIï¼Œé€æ¸è¢« `astream_events` æ›¿ä»£ã€‚

ğŸ‘‰ å»ºè®®ï¼šä¸€èˆ¬ç”¨ `stream`/`astream`ï¼Œéœ€è¦ä¸­é—´æ­¥éª¤å°±ç”¨ `astream_events`ã€‚

---

## 2. è¾“å…¥å’Œè¾“å‡ºç±»å‹ï¼ˆInput / Output Typesï¼‰

æ¯ä¸ª `Runnable` éƒ½æœ‰è¾“å…¥ç±»å‹å’Œè¾“å‡ºç±»å‹ã€‚

| ç»„ä»¶           | è¾“å…¥ç±»å‹                                 | è¾“å‡ºç±»å‹              |
| ------------ | ------------------------------------ | ----------------- |
| Prompt       | dict                                 | PromptValue       |
| ChatModel    | string / chat messages / PromptValue | ChatMessage       |
| LLM          | string / chat messages / PromptValue | string            |
| OutputParser | LLM æˆ– ChatModel çš„è¾“å‡º                  | è‡ªå®šä¹‰ï¼ˆå¦‚ dict, JSONï¼‰ |
| Retriever    | string                               | List\[Document]   |
| Tool         | string æˆ– dict                        | è‡ªå®šä¹‰               |

**ç¤ºä¾‹**

```python
from langchain.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("Translate: {text} â†’ French")

# è¾“å…¥æ˜¯ dict
val = prompt.invoke({"text": "good morning"})
print(type(val))   # PromptValue
```

---

## 3. Inspecting Schemasï¼ˆæ£€æŸ¥è¾“å…¥è¾“å‡ºç»“æ„ï¼‰

âš ï¸ è¿›é˜¶åŠŸèƒ½ï¼Œå¤§å¤šæ•°äººç”¨ä¸åˆ°ï¼Œä½†åœ¨ **è‡ªåŠ¨ç”Ÿæˆ API æ–‡æ¡£ / æ ¡éªŒè¾“å…¥è¾“å‡º** æ—¶æœ‰ç”¨ã€‚

Runnable æä¾›äº† **Pydantic Schema å’Œ JSON Schema** çš„æŸ¥è¯¢æ–¹æ³•ï¼š

* `get_input_schema()`
* `get_output_schema()`
* `config_schema()`
* `get_input_jsonschema()`
* `get_output_jsonschema()`
* `get_config_jsonschema()`

**ç¤ºä¾‹**

```python
schema = llm.get_input_schema()
print(schema.schema_json(indent=2))
```

ç”¨é€”ï¼š

* è‡ªåŠ¨åŒ–æµ‹è¯•
* LangServe è‡ªåŠ¨ç”Ÿæˆ OpenAPI æ–‡æ¡£

---

## 4. with\_typesï¼ˆæ‰‹åŠ¨æŒ‡å®šç±»å‹ï¼‰

LangChain ä¼šå°è¯•æ¨æ–­ Runnable çš„è¾“å…¥è¾“å‡ºç±»å‹ï¼Œä½†åœ¨ **å¤æ‚ LCEL ç»„åˆ** æ—¶å¯èƒ½å‡ºé”™ã€‚
è¿™ç§æƒ…å†µä¸‹å¯ä»¥æ‰‹åŠ¨æŒ‡å®šï¼š

```python
chain = (prompt | llm).with_types(input_type=dict, output_type=str)
```

---

## 5. RunnableConfigï¼ˆè¿è¡Œæ—¶é…ç½®ï¼‰

å‡ ä¹æ‰€æœ‰æ‰§è¡Œæ–¹æ³•ï¼ˆ`invoke/batch/stream/...`ï¼‰éƒ½å¯ä»¥ä¼ ä¸€ä¸ª `config` dictã€‚

å¸¸ç”¨å‚æ•°ï¼š

* `run_name`: è¿è¡Œåï¼ˆä»…çˆ¶çº§ï¼Œä¸ç»§æ‰¿ï¼‰
* `run_id`: è‡ªå®šä¹‰å”¯ä¸€ UUIDï¼ˆè¿½è¸ªï¼‰
* `tags`: æ ‡ç­¾ï¼ˆä¼šç»§æ‰¿åˆ°å­è°ƒç”¨ï¼‰
* `metadata`: å…ƒæ•°æ®ï¼ˆä¼šç»§æ‰¿åˆ°å­è°ƒç”¨ï¼‰
* `callbacks`: å›è°ƒå‡½æ•°ï¼ˆæ—¥å¿—/è¿½è¸ªï¼‰
* `max_concurrency`: å¹¶è¡Œæ•°é™åˆ¶
* `recursion_limit`: é¿å…æ— é™é€’å½’
* `configurable`: è¿è¡Œæ—¶ä¼ å…¥çš„è‡ªå®šä¹‰å‚æ•°

**ç¤ºä¾‹**

```python
result = llm.invoke(
    "Hello",
    config={
        "run_name": "greeting_test",
        "tags": ["demo", "tutorial"],
        "metadata": {"topic": "intro"}
    }
)
```

---

## 6. Propagation of RunnableConfigï¼ˆé…ç½®ä¼ æ’­ï¼‰

Runnable å¯ä»¥ç”±å¤šä¸ªå­ Runnable ç»„æˆï¼ˆå¦‚ prompt | llm | parserï¼‰ã€‚
ä¸ºäº†è®©å­ç»„ä»¶ä¹Ÿèƒ½æ‹¿åˆ° configï¼ˆæ¯”å¦‚å…±äº«å›è°ƒã€metadataï¼‰ï¼ŒLangChain ä¼šè‡ªåŠ¨ä¼ æ’­ configã€‚

**ä¸¤ç§åˆ›å»ºæ¨¡å¼ï¼š**

1. **LCEL å£°æ˜å¼**

   ```python
   chain = prompt | llm | parser
   ```

   ï¼ˆè‡ªåŠ¨ä¼ æ’­ configï¼‰

2. **è‡ªå®šä¹‰ Runnable**ï¼ˆRunnableLambda / @toolï¼‰

   ```python
   from langchain_core.runnables import RunnableLambda

   def foo(input):
       return llm.invoke(input)

   foo_runnable = RunnableLambda(foo)
   ```

âš ï¸ Python ç‰ˆæœ¬å·®å¼‚ï¼š

* **Python 3.11+**ï¼šè‡ªåŠ¨æ”¯æŒ contextvarsï¼Œæ— éœ€æ‰‹åŠ¨ä¼ æ’­ã€‚
* **Python 3.9 / 3.10ï¼ˆå¼‚æ­¥åœºæ™¯ï¼‰**ï¼šéœ€è¦æ‰‹åŠ¨ä¼  `config`ã€‚

**ç¤ºä¾‹ï¼ˆæ‰‹åŠ¨ä¼ æ’­ configï¼‰**

```python
async def foo(input, config):
    return await llm.ainvoke(input, config=config)

foo_runnable = RunnableLambda(foo)
```

---

âœ… æ€»ç»“

* **Streaming APIs**ï¼š`stream / astream`ï¼ˆå¸¸ç”¨ï¼‰ï¼Œ`astream_events`ï¼ˆé«˜çº§ï¼‰ï¼Œ`astream_log`ï¼ˆæ—§ç‰ˆï¼‰ã€‚
* **è¾“å…¥è¾“å‡ºç±»å‹**ï¼šä¸åŒç»„ä»¶ä¸åŒï¼ŒPrompt â†’ dictï¼ŒLLM â†’ stringï¼ŒChatModel â†’ ChatMessageã€‚
* **Schema æ£€æŸ¥**ï¼š`get_input_schema` ç­‰æ–¹æ³•ï¼Œå¤šç”¨äºæµ‹è¯•/æ–‡æ¡£ã€‚
* **RunnableConfig**ï¼šè¿è¡Œæ—¶é…ç½®ï¼ˆrun\_name, tags, metadata, callbacksâ€¦ï¼‰ã€‚
* **é…ç½®ä¼ æ’­**ï¼šåœ¨ LCEL ä¸­è‡ªåŠ¨ä¼ æ’­ï¼Œåœ¨ Python 3.9/3.10 å¼‚æ­¥éœ€æ‰‹åŠ¨ä¼ ã€‚

---

 å¥½çš„ï¼Œæˆ‘ç»§ç»­å¸®ä½ æ•´ç†æˆä¸­æ–‡ç¬”è®°ï¼Œä¿æŒå’Œå‰é¢ä¸€è‡´çš„é£æ ¼ï¼š

---

## âœ… RunnableConfig çš„ä¼ é€’ (Propagation of RunnableConfig)

* å¾ˆå¤š **Runnables** æ˜¯ç”±å…¶ä»– **Runnables** ç»„æˆçš„ï¼Œå› æ­¤å¿…é¡»ç¡®ä¿ **RunnableConfig** å¯ä»¥ä¼ é€’åˆ°æ‰€æœ‰å­è°ƒç”¨ (sub-calls)ã€‚
* è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼šå¯ä»¥åœ¨çˆ¶ Runnable ä¸­è®¾ç½®è¿è¡Œæ—¶é…ç½®ï¼ˆå¦‚ `callbacks`ã€`tags`ã€`metadata`ï¼‰ï¼Œå¹¶è‡ªåŠ¨ç»§æ‰¿åˆ°æ‰€æœ‰å­è°ƒç”¨ã€‚
* å¦‚æœä¸ä¼ é€’ï¼Œå°†æ— æ³•æ­£ç¡®ä¼ æ’­è¿™äº›é…ç½®ï¼Œå¯¼è‡´è°ƒè¯•å’Œè·Ÿè¸ªå›°éš¾ã€‚

### åˆ›å»º Runnables çš„ä¸¤ç§æ–¹å¼

1. **å£°æ˜å¼ (LCEL)**

   ```python
   chain = prompt | chat_model | output_parser
   ```

2. **è‡ªå®šä¹‰ Runnable**

   * ä½¿ç”¨ `RunnableLambda` æˆ– `@tool` è£…é¥°å™¨

   ```python
   def foo(input):
       return bar_runnable.invoke(input)
   foo_runnable = RunnableLambda(foo)
   ```

LangChain ä¼šè‡ªåŠ¨å°è¯•åœ¨ä¸¤ç§æ–¹å¼ä¸‹ä¼ æ’­ `RunnableConfig`ã€‚
å¯¹äºç¬¬äºŒç§æ–¹å¼ï¼Œå®ƒä¾èµ– **Python çš„ contextvars**ï¼š

* **Python 3.11+**ï¼šé»˜è®¤æ”¯æŒï¼Œæ— éœ€é¢å¤–å¤„ç†ã€‚
* **Python 3.9 / 3.10**ï¼šåœ¨å¼‚æ­¥ (async) ä»£ç ä¸­ï¼Œéœ€è¦ **æ‰‹åŠ¨ä¼ é€’** `RunnableConfig`ã€‚

ä¾‹å¦‚ï¼š

```python
async def foo(input, config):  # æ³¨æ„è¿™é‡Œæ¥æ”¶ config
    return await bar_runnable.ainvoke(input, config=config)

foo_runnable = RunnableLambda(foo)
```

âš ï¸ **æ³¨æ„**ï¼š
åœ¨ **Python 3.10 æˆ–æ›´ä½ç‰ˆæœ¬** çš„å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œ`RunnableConfig` æ— æ³•è‡ªåŠ¨ä¼ é€’ï¼Œå°¤å…¶åœ¨ä½¿ç”¨ `astream_events` å’Œ `astream_log` æ—¶è¦å°å¿ƒã€‚

---

## âœ… è®¾ç½®è‡ªå®šä¹‰ Run åç§°ã€æ ‡ç­¾ã€å…ƒæ•°æ®

* åœ¨ `RunnableConfig` ä¸­ï¼Œå¯ä»¥é…ç½®ä»¥ä¸‹å­—æ®µï¼š

  * `run_name`ï¼šå­—ç¬¦ä¸²ï¼Œå½“å‰è°ƒç”¨çš„è‡ªå®šä¹‰åç§°ï¼ˆä¸ä¼šç»§æ‰¿ç»™å­è°ƒç”¨ï¼‰ã€‚
  * `tags`ï¼šåˆ—è¡¨ï¼Œç”¨äºæ·»åŠ æ ‡ç­¾ï¼Œä¼šç»§æ‰¿ç»™å­è°ƒç”¨ã€‚
  * `metadata`ï¼šå­—å…¸ï¼Œç”¨äºæ·»åŠ å…ƒæ•°æ®ï¼Œä¼šç»§æ‰¿ç»™å­è°ƒç”¨ã€‚

ç”¨é€”ï¼š

* åœ¨ **LangSmith** ä¸­å¯ä»¥ç”¨äºè°ƒè¯•ã€è·Ÿè¸ªã€è¿‡æ»¤ã€‚
* ä¼šåœ¨ **å›è°ƒ (callbacks)** å’Œ **æµå¼ API (astream\_events)** ä¸­æ˜¾ç¤ºã€‚

---

## âœ… è®¾ç½® run\_id

* é«˜çº§åŠŸèƒ½ï¼ˆå¤§éƒ¨åˆ†ç”¨æˆ·æ— éœ€ä½¿ç”¨ï¼‰ã€‚
* `run_id` å¿…é¡»æ˜¯ **UUID å­—ç¬¦ä¸²**ï¼Œä¸”å¯¹æ¯ä¸ªè¿è¡Œå”¯ä¸€ã€‚
* ç”¨äºå”¯ä¸€æ ‡è¯†è°ƒç”¨ï¼Œä¾¿äºè·¨ç³»ç»Ÿå…³è”ã€‚

ç¤ºä¾‹ï¼š

```python
import uuid

run_id = uuid.uuid4()
some_runnable.invoke(
   some_input,
   config={"run_id": run_id}
)
```

---

## âœ… è®¾ç½®é€’å½’é™åˆ¶ (recursion\_limit)

* ä¸€äº› Runnables å¯èƒ½ä¼šè¿”å›æ–°çš„ Runnablesï¼Œå¯¼è‡´é€’å½’è°ƒç”¨ã€‚
* ä¸ºé¿å…æ— é™é€’å½’ï¼Œå¯ä»¥åœ¨ `RunnableConfig` ä¸­è®¾ç½® `recursion_limit`ã€‚

---

## âœ… è®¾ç½®æœ€å¤§å¹¶å‘ (max\_concurrency)

* åœ¨ `batch` æˆ– `batch_as_completed` ä¸­ä½¿ç”¨ã€‚
* æ§åˆ¶æœ€å¤§å¹¶è¡Œè°ƒç”¨æ•°ï¼Œé˜²æ­¢è¿‡è½½ã€‚

ğŸ‘‰ æ›´æ¨èä½¿ç”¨ **LangChain å†…ç½®çš„é€Ÿç‡é™åˆ¶å™¨** æ¥å¤„ç†è¯·æ±‚é€Ÿç‡ï¼Œè€Œä¸æ˜¯ `max_concurrency`ã€‚

---

## âœ… è®¾ç½® configurable

* ç”¨äºä¼ é€’è¿è¡Œæ—¶å‚æ•°ã€‚
* å¸¸è§åœºæ™¯ï¼š

  * **LangGraph** ä¸­çš„æŒä¹…åŒ–ä¸è®°å¿†åŠŸèƒ½ã€‚
  * **RunnableWithMessageHistory** ä¸­æŒ‡å®š `session_id` æˆ– `conversation_id`ã€‚
  * è‡ªå®šä¹‰å‚æ•°ä¼ é€’ç»™ Configurable Runnableã€‚

---

## âœ… è®¾ç½® callbacks

* å¯ä»¥åœ¨è¿è¡Œæ—¶é…ç½®å›è°ƒï¼Œå›è°ƒä¼šä¼ é€’ç»™æ‰€æœ‰å­è°ƒç”¨ã€‚

ç¤ºä¾‹ï¼š

```python
some_runnable.invoke(
   some_input,
   {
      "callbacks": [
         SomeCallbackHandler(),
         AnotherCallbackHandler(),
      ]
   }
)
```

---

## âœ… ä»å‡½æ•°åˆ›å»º Runnable

ä¸¤ç§æ–¹å¼ï¼š

* `RunnableLambda`ï¼šé€‚ç”¨äºç®€å•é€»è¾‘ï¼ˆä¸éœ€è¦æµå¼ï¼‰ã€‚
* `RunnableGenerator`ï¼šé€‚ç”¨äºéœ€è¦æµå¼çš„å¤æ‚é€»è¾‘ã€‚

ğŸ‘‰ ä¸æ¨èç›´æ¥ç»§æ‰¿ Runnables åˆ›å»ºæ–°ç±»ï¼Œå¤æ‚ä¸”å®¹æ˜“å‡ºé”™ã€‚

---

## âœ… å¯é…ç½® Runnables (Configurable Runnables)

* é«˜çº§åŠŸèƒ½ï¼Œä¸»è¦ç”¨äº LCEL ç»„åˆçš„å¤§é“¾ (chains)ï¼Œä»¥åŠ **LangServe éƒ¨ç½²**ã€‚
* æä¾›ä¸¤ç±»æ–¹æ³•ï¼š

  * `configurable_fields`ï¼šé…ç½®æŸä¸ªå±æ€§ï¼ˆå¦‚ ChatModel çš„ `temperature`ï¼‰ã€‚
  * `configurable_alternatives`ï¼šåœ¨å¤šä¸ª Runnables ä¹‹é—´åˆ‡æ¢ï¼ˆå¦‚ä¸åŒçš„ ChatModelï¼‰ã€‚

---

 