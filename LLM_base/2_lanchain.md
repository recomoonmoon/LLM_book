
---

# ğŸ“˜ LangChain + Tongyi ä½¿ç”¨ç¬”è®°

## ğŸ“– å‚è€ƒèµ„æ–™

* [LangChain ä¸­æ–‡æ–‡æ¡£](https://python.langchain.ac.cn/docs/introduction/)
* [å´æ©è¾¾ã€ŠLLM Cookbookã€‹æ•™ç¨‹](https://datawhalechina.github.io/llm-cookbook/#/C2/readme)

---

## ğŸš€ ç¯å¢ƒå‡†å¤‡

åœ¨å›½å†…ç¯å¢ƒä¸‹ï¼Œä¸ä½¿ç”¨ OpenAI çš„ keyï¼Œè€Œæ˜¯ä½¿ç”¨ **é€šä¹‰åƒé—® (Qwen / Tongyi)** çš„ API keyã€‚

1. åœ¨ `record.env` æ–‡ä»¶ä¸­é…ç½®ç¯å¢ƒå˜é‡ï¼š
   ```env
   QWEN_API_KEY=your_qwen_key
   QWEN_URL=your_qwen_url
   ```

2. åœ¨ä»£ç é‡ŒåŠ è½½å¹¶è®¾ç½®ï¼š

   ```python
   from dotenv import load_dotenv
   import os

   if load_dotenv("../record.env"):
       os.environ["DASHSCOPE_API_KEY"] = os.environ["QWEN_API_KEY"]
   ```

---

## âš™ï¸ åŸºç¡€è°ƒç”¨

æ–°ç‰ˆ LangChain å·²å¼ƒç”¨ `langchain.llms`ï¼Œåº”ä½¿ç”¨ `langchain_community.llms.Tongyi`ï¼š

```python
from langchain_community.llms import Tongyi

llm = Tongyi(model="qwen-turbo", temperature=0.7)
print(llm.invoke("hello world"))
```

---

## ğŸ”— ä½¿ç”¨ PromptTemplate + Chain

å°è£…ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼ŒæŠŠ **æ¨¡æ¿** å’Œ **æ¨¡å‹** ä¸²æˆä¸€ä¸ª chainï¼š

```python
def invoke_and_chain(model, temperature, prompt_p, var_dict):
    client = Tongyi(model=model, temperature=temperature)
    prompt = PromptTemplate.from_template(prompt_p)
    chain = prompt | client
    return chain.invoke(var_dict)
```

è°ƒç”¨ç¤ºä¾‹ï¼š

```python
model = "qwen-turbo"
temperature = 0.7
prompt_p = '''
{var1} * {var2} ç­‰äºå¤šå°‘ï¼Ÿ
'''
vardict = {"var1": 5, "var2": 7}

response = invoke_and_chain(
    model=model, 
    temperature=temperature, 
    prompt_p=prompt_p, 
    var_dict=vardict
)
print(response)
```

è¾“å‡ºï¼š

```
35
```

---

## ğŸ’¡ æç¤ºæŠ€å·§

* `PromptTemplate` æ”¯æŒå˜é‡æ’å€¼ï¼Œå¯å¿«é€Ÿå¤ç”¨æ¨¡æ¿ã€‚
* ä½¿ç”¨ `"Let's think step by step"` ç­‰æ€ç»´é“¾æç¤ºï¼Œå¯ä»¥æå‡æ¨ç†å‡†ç¡®ç‡ã€‚
* å¯¹äºå¯¹è¯å¼åœºæ™¯ï¼Œå¯æ”¹ç”¨ `ChatTongyi` + `SystemMessage/HumanMessage`ã€‚

---

 