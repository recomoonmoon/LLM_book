 
---

# Byte-Level BPE Tokenizer

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ª **å­—èŠ‚çº§ BPEï¼ˆByte Pair Encodingï¼‰åˆ†è¯å™¨è®­ç»ƒå™¨**ï¼Œæ”¯æŒé«˜æ•ˆçš„å¹¶è¡Œé¢„åˆ†è¯ä¸åˆå¹¶æ“ä½œä¼˜åŒ–ã€‚è¯¥å®ç°åŸºäº **Stanford CS336 ä½œä¸š**ï¼Œå¹¶åœ¨ TinyStories æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚

---

## ğŸ“Œ åŠŸèƒ½ç‰¹æ€§

* **å­—èŠ‚çº§åˆå§‹åŒ–**ï¼šä»¥æ‰€æœ‰ 0-255 çš„å­—èŠ‚ä½œä¸ºåˆå§‹è¯è¡¨ã€‚
* **ç‰¹æ®Šç¬¦å·å¤„ç†**ï¼šæ”¯æŒæ·»åŠ  `<|endoftext|>`ã€`<pad>` ç­‰ç”¨æˆ·å®šä¹‰çš„ special tokensã€‚
* **å¹¶è¡Œé¢„åˆ†è¯**ï¼š

  * åŸºäºæ–‡ä»¶è¾¹ç•Œ `<|endoftext|>` è¿›è¡Œåˆ‡åˆ†ï¼Œé¿å…è·¨æ–‡æ¡£ã€‚
  * ä½¿ç”¨ `multiprocessing.Pool` åŠ é€Ÿå¤„ç†ã€‚
* **é«˜æ•ˆ BPE åˆå¹¶**ï¼š

  * ä»…æ›´æ–°å—å½±å“çš„ pair é¢‘ç‡ï¼Œé¿å…å…¨å±€ç»Ÿè®¡ã€‚
  * ä½¿ç”¨ `pair_to_indices` æ˜ å°„ç®¡ç† pair â†’ token çš„ç´¢å¼•é›†åˆã€‚
* **æœ€ç»ˆè¾“å‡º**ï¼š

  * `vocab: dict[int, bytes]` â€” è¯è¡¨ï¼Œtoken ID åˆ°å­—èŠ‚çš„æ˜ å°„ã€‚
  * `merges: list[tuple[bytes, bytes]]` â€” è®­ç»ƒè¿‡ç¨‹ä¸­æ‰§è¡Œçš„åˆå¹¶æ“ä½œã€‚

---

## âš™ï¸ ä¸»è¦æµç¨‹

1. **åˆå§‹åŒ–è¯è¡¨**

   ```python
   vocab = {i: bytes([i]) for i in range(256)}
   for tok in special_tokens:
       vocab[len(vocab)] = tok.encode("utf-8")
   ```

2. **å¹¶è¡Œé¢„åˆ†è¯**

   * ä½¿ç”¨ `find_chunk_boundaries` æ‰¾åˆ° `<|endoftext|>` è¾¹ç•Œã€‚
   * è°ƒç”¨ `process_chunk` å¯¹æ¯ä¸ª chunk åˆ†è¯å¹¶è½¬ä¸ºå­—èŠ‚ã€‚

3. **BPE è®­ç»ƒ**

   * ç»Ÿè®¡ pair å‡ºç°é¢‘ç‡ `counts`ã€‚
   * é€‰æ‹©é¢‘ç‡æœ€é«˜çš„ pair `(a, b)`ï¼Œæ‰§è¡Œåˆå¹¶å¹¶æ›´æ–° `vocab`ã€‚
   * æ›´æ–°å—å½±å“ token çš„ pair è®¡æ•°ã€‚
   * é‡å¤ï¼Œç›´åˆ° `vocab_size` è¾¾åˆ°ç›®æ ‡ã€‚

---

## ğŸ“‚ æ ¸å¿ƒæ¨¡å—

* `train_bpe` â€” ä¸»å…¥å£å‡½æ•°
* `find_chunk_boundaries` â€” æ•°æ®é›†åˆ‡åˆ†
* `process_chunk` â€” é¢„åˆ†è¯
* **è¾“å‡º**ï¼š`vocab`, `merges`

---

## ğŸ§ª æµ‹è¯•æ–¹æ³•

```bash
uv run pytest tests/test_train_bpe.py
```

ç¡®ä¿å®ç°èƒ½é€šè¿‡ä¸‰ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼š

* è¯è¡¨æ„é€ 
* ç‰¹æ®Š token è¡Œä¸º
* åˆå¹¶é¡ºåºæ­£ç¡®æ€§

---
## ä½œä¸šï¼š
 * å®Œæˆæœ¬ç›®å½•ä¸‹test.pyçš„ä»£ç ï¼Œå®ç°Tokenizerï¼ˆå®Œæ•´ç‰ˆTokenizer.pyï¼‰ã€‚
 * å¦‚æœæœ‰ä½™åŠ›ï¼Œå¯ä»¥å®ç°å…¶è®­ç»ƒè¿‡ç¨‹

---

## ğŸ“– å‚è€ƒ

* Stanford CS336 ä½œä¸šè¯´æ˜
* [Sennrich et al. (2016) - Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)
* HuggingFace `tokenizers` æºç 

---

 