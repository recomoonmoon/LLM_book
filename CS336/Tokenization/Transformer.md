 
---

# ğŸ“˜ Transformer Language Model (TransformerLM) å®ç°ç¬”è®°

æœ¬æ–‡å®ç°äº†ä¸€ä¸ª **ç®€åŒ–ç‰ˆ Transformer è¯­è¨€æ¨¡å‹**ï¼ŒåŒ…æ‹¬ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š

1. **Linear**: è‡ªå®šä¹‰çº¿æ€§å±‚ï¼ˆæ—  biasï¼‰ã€‚
2. **Embedding**: è¯åµŒå…¥å±‚ï¼Œå°† token id è½¬æ¢ä¸ºå‘é‡ã€‚
3. **RMSNorm**: Root Mean Square LayerNormï¼Œä»£æ›¿ä¼ ç»Ÿ LayerNormã€‚
4. **SwiGLU**: å‰é¦ˆç½‘ç»œçš„æ¿€æ´»å‡½æ•°æ”¹è¿›ç‰ˆæœ¬ã€‚
5. **RoPE (Rotary Positional Embedding)**: æ—‹è½¬ä½ç½®ç¼–ç ã€‚
6. **Multi-Head Self-Attention**: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚
7. **TransformerBlock**: åŸºç¡€ Transformer å—ï¼ˆæ³¨æ„åŠ› + FFNï¼‰ã€‚
8. **TransformerLM**: å®Œæ•´çš„è¯­è¨€æ¨¡å‹ã€‚

---

## ğŸ”¹ Linear

```python
class Linear(nn.Module):
    def __init__(self, in_features, out_features, ...):
        # çº¿æ€§å±‚ï¼ˆæ— åç½®ï¼‰
        # y = x @ W^T
```

* ä½¿ç”¨ `einsum` å®ç°çŸ©é˜µä¹˜æ³•ã€‚
* æƒé‡é‡‡ç”¨æˆªæ–­æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œä¿è¯ç¨³å®šã€‚

---

## ğŸ”¹ Embedding

```python
class Embedding(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, ...):
        # å°† token_id â†’ å‘é‡è¡¨ç¤º (d_model)
```

* è¾“å…¥ `(batch_size, seq_len)`
* è¾“å‡º `(batch_size, seq_len, d_model)`

---

## ğŸ”¹ RMSNorm

```python
class RMSNorm(nn.Module):
    def forward(self, x):
        # è®¡ç®—å‡æ–¹æ ¹å½’ä¸€åŒ–
        rms = (x.pow(2).mean(-1, keepdim=True) + eps).sqrt()
        return x / rms * weight
```

* ä¸ LayerNorm ä¸åŒï¼Œ**ä¸å‡å‡å€¼**ï¼ŒåªæŒ‰ RMS å½’ä¸€åŒ–ã€‚
* æ•°å€¼æ›´ç¨³å®šï¼Œå¸¸ç”¨äº GPT ç³»åˆ—ã€‚

---

## ğŸ”¹ SwiGLU

```python
class SwiGLU(nn.Module):
    def forward(self, x):
        return W2( SiLU(W1(x)) * W3(x) )
```

* ä»£æ›¿ ReLU çš„ **é—¨æ§æ¿€æ´»å‡½æ•°**ã€‚
* **ä¼˜åŠ¿**ï¼šæé«˜è¡¨è¾¾èƒ½åŠ›ï¼Œå·²ç”¨äº LLaMAã€PaLMã€‚

---

## ğŸ”¹ RoPE (Rotary Positional Embedding)

```python
class ROPE(nn.Module):
    def forward(self, x, token_positions):
        # å°† cos/sin æ—‹è½¬åµŒå…¥åˆ° query/key å‘é‡ä¸­
```

* ä½¿ç”¨ **æ­£ä½™å¼¦å‡½æ•°**è®©æ¨¡å‹å­¦ä¹ åºåˆ—ä½ç½®ã€‚
* **å¥½å¤„**ï¼šæ”¯æŒæ— é™é•¿ extrapolationï¼Œæ¯”ç»å¯¹ä½ç½®ç¼–ç æ›´çµæ´»ã€‚

---

## ğŸ”¹ Multi-Head Self Attention

```python
class MultiHeadSelfAttention(nn.Module):
    def forward(self, x, mask=None, token_positions=None):
        q = Wq(x), k = Wk(x), v = Wv(x)
        q, k, v = å¤´åˆ†æ‹†
        q, k = RoPE(q, k)
        attn = softmax(q @ k^T / sqrt(d_k))
        out = attn @ v
```

* å¤šå¤´æ³¨æ„åŠ›ï¼Œæ”¯æŒ **RoPE**ã€‚
* é»˜è®¤ **causal mask**ï¼Œä¿è¯åªçœ‹è§è¿‡å»çš„ tokenã€‚

---

## ğŸ”¹ TransformerBlock

```python
class TransformerBlock(nn.Module):
    def forward(self, x):
        x = x + Attention(LN(x))
        x = x + FFN(LN(x))
        return x
```

* æ®‹å·®ç»“æ„ï¼š**å‰å½’ä¸€åŒ– + æ®‹å·®è¿æ¥**ã€‚
* å…¸å‹çš„ **Transformer ç»“æ„å•å…ƒ**ã€‚

---

## ğŸ”¹ TransformerLM

```python
class TransformerLM(nn.Module):
    def forward(self, inputs):
        x = Embedding(inputs)
        for block in layers:
            x = block(x)
        logits = lm_head(LN(x))
        return logits
```

* è¾“å…¥ï¼š`(batch_size, seq_len)`
* è¾“å‡ºï¼š`(batch_size, seq_len, vocab_size)`
* é¢„æµ‹æ¯ä¸ªä½ç½®çš„ä¸‹ä¸€ä¸ª tokenã€‚

---

## ğŸ”¹ æ¨¡å‹ç»“æ„ç¤ºæ„

```plaintext
è¾“å…¥ tokens â†’ Embedding â†’ [Block Ã— N] â†’ RMSNorm â†’ Linear(vocab_size)
```

å…¶ä¸­æ¯ä¸ª **Block** å†…éƒ¨æ˜¯ï¼š

```plaintext
x â†’ RMSNorm â†’ MultiHeadSelfAttention â†’ æ®‹å·®
x â†’ RMSNorm â†’ SwiGLU FeedForward â†’ æ®‹å·®
```

---

## ğŸ”¹ æ€»ç»“

* æœ¬å®ç°å±äº **ç®€åŒ–ç‰ˆ GPT æ¶æ„**ï¼š

  * ä½¿ç”¨ **RMSNorm + SwiGLU + RoPE** â†’ æ›´æ¥è¿‘ LLaMA é£æ ¼ã€‚
  * è¾“å‡º logitsï¼Œä¸åœ¨ forward å†…éƒ¨åš softmaxï¼Œ**æ–¹ä¾¿è®­ç»ƒæ—¶é…åˆ CrossEntropyLoss**ã€‚
* å¯æ‰©å±•ï¼šå¢åŠ  `num_layers`ã€`num_heads`ã€`d_model` å³å¯å®ç°æ›´å¤§æ¨¡å‹ã€‚

---

 